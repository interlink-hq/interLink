---
name: integration-test-k3s
permissions:
  contents: read

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  integration-test:
    name: Integration Test (K3s)
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'

      - name: Run golangci-lint
        uses: golangci/golangci-lint-action@v7
        with:
          version: v2.1
          args: --timeout=30m

      - name: Run unit tests
        run: |
          echo "Running unit tests..."
          go test -v -cover ./pkg/... || exit 1
          echo "Unit tests passed"

      - name: Set up K3s
        uses: debianmaster/actions-k3s@master
        id: k3s
        with:
          version: 'v1.31.4-k3s1'

      - name: Wait for K3s to be ready
        run: |
          echo "Waiting for K3s cluster to be ready..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s
          kubectl get nodes
          kubectl get pods -A

      - name: Create Virtual Kubelet service account
        run: |
          echo "Creating service account and RBAC for Virtual Kubelet..."
          kubectl apply -f - <<EOF
          apiVersion: v1
          kind: ServiceAccount
          metadata:
            name: virtual-kubelet
            namespace: default
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRole
          metadata:
            name: virtual-kubelet
          rules:
          - apiGroups:
            - "coordination.k8s.io"
            resources:
            - leases
            verbs:
            - update
            - create
            - get
            - list
            - watch
            - patch
          - apiGroups:
            - ""
            resources:
            - configmaps
            - secrets
            - services
            - serviceaccounts
            - namespaces
            verbs:
            - get
            - list
            - watch
          - apiGroups: [""]
            resources: ["serviceaccounts/token"]
            verbs:
            - create
            - get
            - list
          - apiGroups:
            - ""
            resources:
            - pods
            verbs:
            - delete
            - get
            - list
            - watch
            - patch
          - apiGroups:
            - ""
            resources:
            - nodes
            verbs:
            - create
            - get
          - apiGroups:
            - ""
            resources:
            - nodes/status
            verbs:
            - update
            - patch
          - apiGroups:
            - ""
            resources:
            - pods/status
            verbs:
            - update
            - patch
          - apiGroups:
            - ""
            resources:
            - events
            verbs:
            - create
            - patch
          - apiGroups:
            - "certificates.k8s.io"
            resources:
            - certificatesigningrequests
            verbs:
            - create
            - get
            - list
            - watch
            - delete
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: virtual-kubelet
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: virtual-kubelet
          subjects:
          - kind: ServiceAccount
            name: virtual-kubelet
            namespace: default
          EOF

          echo "Waiting for service account to be ready..."
          sleep 5
          kubectl get sa virtual-kubelet -n default

      - name: Build Docker images
        run: |
          echo "Building interLink API Docker image..."
          docker build -f docker/Dockerfile.interlink -t interlink:ci-test .

          echo "Cloning SLURM plugin repository..."
          git clone https://github.com/interlink-hq/interlink-slurm-plugin.git /tmp/slurm-plugin

          echo "Building SLURM plugin Docker image..."
          docker build -t interlink-slurm-plugin:ci-test /tmp/slurm-plugin

          echo "Docker images built successfully"
          docker images | grep -E "interlink|slurm"

      - name: Create configuration files
        run: |
          mkdir -p /tmp/interlink

          echo "Creating plugin config..."
          cat > /tmp/interlink/plugin-config.yaml <<EOF
          InterlinkURL: "http://localhost"
          InterlinkPort: "3000"
          SidecarURL: "http://0.0.0.0"
          SidecarPort: "4000"
          VerboseLogging: true
          ErrorsOnlyLogging: false
          DataRootFolder: "/tmp/.interlink/"
          ExportPodData: true
          EOF

          echo "Creating interLink config..."
          cat > /tmp/interlink/interlink-config.yaml <<EOF
          InterlinkAddress: "http://0.0.0.0"
          InterlinkPort: "3000"
          SidecarURL: "http://localhost"
          SidecarPort: "4000"
          VerboseLogging: true
          ErrorsOnlyLogging: false
          DataRootFolder: "/tmp/.interlink"
          EOF

      - name: Start SLURM plugin container
        run: |
          echo "Starting SLURM plugin container..."
          docker run -d --name interlink-plugin \
            -p 4000:4000 --privileged \
            -v /tmp/interlink/plugin-config.yaml:/etc/interlink/InterLinkConfig.yaml \
            -e SHARED_FS=true \
            -e SLURMCONFIGPATH=/etc/interlink/InterLinkConfig.yaml \
            interlink-slurm-plugin:ci-test

          sleep 5

          if ! docker ps | grep -q interlink-plugin; then
            echo "ERROR: Plugin container failed to start!"
            docker logs interlink-plugin
            exit 1
          fi

          echo "SLURM plugin container started successfully"
          docker ps | grep interlink-plugin

      - name: Start interLink API container
        run: |
          echo "Starting interLink API container..."
          docker run -d --name interlink-api \
            -p 3000:3000 \
            -v /tmp/interlink/interlink-config.yaml:/etc/interlink/InterLinkConfig.yaml \
            -e INTERLINKCONFIGPATH=/etc/interlink/InterLinkConfig.yaml \
            interlink:ci-test

          sleep 5

          if ! docker ps | grep -q interlink-api; then
            echo "ERROR: interLink API container failed to start!"
            docker logs interlink-api
            exit 1
          fi

          # Test interLink connectivity
          echo "Testing interLink API connectivity..."
          for i in {1..10}; do
            if curl -f http://localhost:3000/status 2>/dev/null; then
              echo "interLink API is responding"
              break
            fi
            echo "Waiting for interLink API... ($i/10)"
            sleep 2
          done

          echo "interLink API container started successfully"
          docker ps | grep interlink-api

      - name: Start Virtual Kubelet
        run: |
          echo "Starting Virtual Kubelet on host..."

          export NODENAME=virtual-kubelet
          export KUBELET_PORT=10250
          export KUBELET_URL=0.0.0.0

          # Cross-platform IP detection
          if command -v hostname &> /dev/null && hostname -I &> /dev/null 2>&1; then
            # Linux
            export POD_IP=$(hostname -I | awk '{print $1}')
          else
            # macOS or other Unix
            export POD_IP=$(ifconfig | grep "inet " | grep -v 127.0.0.1 | head -1 | awk '{print $2}')
          fi

          export CONFIGPATH=$PWD/scripts/virtual-kubelet-config.yaml

          # Create token-based kubeconfig for Virtual Kubelet
          echo "Creating token-based kubeconfig..."

          # Get service account token
          VK_TOKEN=$(kubectl create token virtual-kubelet -n default --duration=24h --kubeconfig=/etc/rancher/k3s/k3s.yaml)

          # Get cluster info from current kubeconfig
          K8S_SERVER=$(kubectl config view --minify -o jsonpath='{.clusters[0].cluster.server}' --kubeconfig=/etc/rancher/k3s/k3s.yaml)

          # Check if certificate-authority-data exists, otherwise use certificate-authority file
          K8S_CA_DATA=$(kubectl config view --minify --raw -o jsonpath='{.clusters[0].cluster.certificate-authority-data}' --kubeconfig=/etc/rancher/k3s/k3s.yaml)
          K8S_CA_FILE=$(kubectl config view --minify --raw -o jsonpath='{.clusters[0].cluster.certificate-authority}' --kubeconfig=/etc/rancher/k3s/k3s.yaml)

          if [ -n "$K8S_CA_DATA" ]; then
            # Use certificate-authority-data if available
            cat > /tmp/vk-kubeconfig.yaml <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - name: default-cluster
            cluster:
              server: ${K8S_SERVER}
              certificate-authority-data: ${K8S_CA_DATA}
          contexts:
          - name: default-context
            context:
              cluster: default-cluster
              user: virtual-kubelet
              namespace: default
          current-context: default-context
          users:
          - name: virtual-kubelet
            user:
              token: ${VK_TOKEN}
          EOF
          elif [ -n "$K8S_CA_FILE" ] && [ -f "$K8S_CA_FILE" ]; then
            # Use certificate-authority file and encode it (cross-platform base64)
            if base64 --help 2>&1 | grep -q -- '-w'; then
              K8S_CA_DATA=$(cat "$K8S_CA_FILE" | base64 -w 0)
            else
              K8S_CA_DATA=$(cat "$K8S_CA_FILE" | base64)
            fi
            cat > /tmp/vk-kubeconfig.yaml <<EOF
          apiVersion: v1
          kind: Config
          clusters:
          - name: default-cluster
            cluster:
              server: ${K8S_SERVER}
              certificate-authority-data: ${K8S_CA_DATA}
          contexts:
          - name: default-context
            context:
              cluster: default-cluster
              user: virtual-kubelet
              namespace: default
          current-context: default-context
          users:
          - name: virtual-kubelet
            user:
              token: ${VK_TOKEN}
          EOF
          else
            echo "ERROR: Could not find CA certificate"
            exit 1
          fi

          export KUBECONFIG=/tmp/vk-kubeconfig.yaml

          echo "Virtual Kubelet environment:"
          echo "  NODENAME: $NODENAME"
          echo "  KUBELET_PORT: $KUBELET_PORT"
          echo "  POD_IP: $POD_IP"
          echo "  CONFIGPATH: $CONFIGPATH"
          echo "  KUBECONFIG: $KUBECONFIG (token-based)"

          go run ./cmd/virtual-kubelet/main.go > /tmp/vk.log 2>&1 &
          echo $! > /tmp/vk.pid

          echo "Virtual Kubelet started (PID: $(cat /tmp/vk.pid))"

          # Wait for VK to register with K8s
          sleep 15

          echo "Checking for virtual-kubelet node..."
          kubectl get nodes

          if ! kubectl get node virtual-kubelet 2>/dev/null; then
            echo "WARNING: Virtual Kubelet node not found yet"
            echo "Virtual Kubelet logs:"
            tail -50 /tmp/vk.log
          fi

      - name: Approve CSRs for kubectl logs support
        run: |
          echo "Approving Certificate Signing Requests..."
          sleep 5
          kubectl get csr
          kubectl get csr -o name | xargs -r kubectl certificate approve || true
          kubectl get csr

      - name: Install test framework
        run: |
          echo "Installing vk-test-set..."
          git clone https://github.com/interlink-hq/vk-test-set.git /tmp/vk-test-set
          cd /tmp/vk-test-set

          # Create test config
          cat > vktest_config.yaml <<EOF
          target_nodes:
            - virtual-kubelet

          required_namespaces:
            - default
            - kube-system

          timeout_multiplier: 10.
          values:
            namespace: default

            tolerations:
              - key: virtual-node.interlink/no-schedule
                operator: Exists
                effect: NoSchedule
          EOF

          # Setup Python virtual environment and install (matching ci/main.go)
          echo "Setting up Python environment..."
          python3 -m venv .venv
          source .venv/bin/activate
          pip3 install -e ./
          echo "vk-test-set installed successfully"

      - name: Wait for Virtual Kubelet to be ready
        run: |
          echo "Waiting for virtual-kubelet node to be ready..."
          for i in {1..30}; do
            if kubectl get node virtual-kubelet 2>/dev/null; then
              echo "Virtual Kubelet node found!"
              kubectl get node virtual-kubelet -o wide
              break
            fi
            echo "Waiting for virtual-kubelet node... ($i/30)"
            sleep 5
          done

          if ! kubectl get node virtual-kubelet; then
            echo "ERROR: Virtual Kubelet node not available after waiting"
            echo "Virtual Kubelet logs:"
            cat /tmp/vk.log
            exit 1
          fi

      - name: Run integration tests
        run: |
          echo "Running integration tests..."
          cd /tmp/vk-test-set

          # Activate venv and run pytest (matching ci/main.go pattern)
          source .venv/bin/activate
          export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
          pytest -v -k "not rclone and not limits" || true

          echo "Test execution completed"

      - name: Collect logs on failure
        if: failure()
        run: |
          echo "=== SLURM plugin container logs ==="
          docker logs interlink-plugin || echo "No plugin logs found"

          echo ""
          echo "=== interLink API container logs ==="
          docker logs interlink-api || echo "No interLink logs found"

          echo ""
          echo "=== Virtual Kubelet logs ==="
          cat /tmp/vk.log || echo "No VK logs found"

          echo ""
          echo "=== Kubernetes pods ==="
          kubectl get pods -A

          echo ""
          echo "=== Kubernetes nodes ==="
          kubectl get nodes -o wide

          echo ""
          echo "=== Kubernetes events ==="
          kubectl get events -A --sort-by='.lastTimestamp'

          echo ""
          echo "=== Docker containers ==="
          docker ps -a

      - name: Cleanup
        if: always()
        run: |
          echo "Stopping Virtual Kubelet..."
          kill $(cat /tmp/vk.pid) 2>/dev/null || true

          echo "Stopping Docker containers..."
          docker stop interlink-api interlink-plugin 2>/dev/null || true
          docker rm interlink-api interlink-plugin 2>/dev/null || true

          echo "Cleanup completed"
